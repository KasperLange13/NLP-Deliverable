{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59c7956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaspe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaspe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaspe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kaspe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936594e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "path = \"./twitter_training.csv\"\n",
    "colnames = [\"Tweet_ID\",\"Entity\",\"Sentiment\",\"Tweet_Text\"]\n",
    "\n",
    "training   = pd.read_csv(path, names = colnames, header= None)\n",
    "validation = pd.read_csv(path, names = colnames, header= None)\n",
    "\n",
    "# pre-process data\n",
    "training   = training.dropna()\n",
    "training   = training[training[\"Sentiment\"] != \"Irrelevant\"]\n",
    "training   = training[training[\"Sentiment\"] != \"Neutral\"]\n",
    "validation = validation.dropna()\n",
    "validation   = validation[validation[\"Sentiment\"] != \"Irrelevant\"]\n",
    "validation   = validation[validation[\"Sentiment\"] != \"Neutral\"]\n",
    "\n",
    "sentiment = {'Positive':1,'Negative':0}\n",
    "\n",
    "training.Sentiment = [sentiment[item] for item in training.Sentiment]\n",
    "validation.Sentiment = [sentiment[item] for item in validation.Sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2509f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for the results\n",
    "results = pd.DataFrame(columns=[\"Type\",\"Precision\",\"Recall\",\"F1_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f593ce5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22358\n",
       "1    20655\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for bias in dataset\n",
    "training[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26a6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(lowercase= False)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_base = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"Baseline\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_base),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_base),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_base)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b547dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LowerCase\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(lowercase= True)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_Lower_Case = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"LowerCase\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_Lower_Case),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_Lower_Case),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_Lower_Case)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a383ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "\n",
    "# List of English stop words to be removed\n",
    "lst_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(lowercase= False, stop_words= lst_stopwords)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_Stop_Words = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"StopWords\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_Stop_Words),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_Stop_Words),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_Stop_Words)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ceed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming \n",
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    #if lst_stopwords is not None:\n",
    "    #    lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "        \n",
    "    lst_text = [ps.stem(word) for word in lst_text]\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "training[\"Tweet_Text_stem\"] = training[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "validation[\"Tweet_Text_stem\"] = validation[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(lowercase= False)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text_stem'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text_stem'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_lemma = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"Stemming\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_lemma),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_lemma),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_lemma)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd383a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lowercase + stopwords\n",
    "\n",
    "training[\"Tweet_Text_stem\"] = training[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "validation[\"Tweet_Text_stem\"] = validation[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(stop_words= lst_stopwords)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text_stem'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text_stem'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_lemma = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"LowerCase X Stopwords\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_lemma),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_lemma),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_lemma)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7e48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lowercase + stopwords\n",
    "\n",
    "training[\"Tweet_Text_stem\"] = training[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "validation[\"Tweet_Text_stem\"] = validation[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(stop_words= lst_stopwords)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text_stem'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text_stem'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_lemma = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"LowerCase X Stopwords\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_lemma),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_lemma),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_lemma)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4bd1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LowerCase + Stemming\n",
    "\n",
    "def stemming(text):\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    #if lst_stopwords is not None:\n",
    "    #    lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "        \n",
    "    lst_text = [ps.stem(word) for word in lst_text]\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "training[\"Tweet_Text_stem\"] = training[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "validation[\"Tweet_Text_stem\"] = validation[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text_stem'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text_stem'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_lemma = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"LowerCase X Stemming\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_lemma),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_lemma),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_lemma)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62c34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stopword + Stemming\n",
    "\n",
    "def stemming(text):\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "        \n",
    "    lst_text = [ps.stem(word) for word in lst_text]\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "training[\"Tweet_Text_stem\"] = training[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "validation[\"Tweet_Text_stem\"] = validation[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer(lowercase= False)\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text_stem'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text_stem'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_lemma = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"Stopwords X Stemming\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_lemma),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_lemma),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_lemma)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259d2b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stopwords + Stemming + Lowercase\n",
    "\n",
    "def stemming(text):\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "        \n",
    "    lst_text = [ps.stem(word) for word in lst_text]\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "training[\"Tweet_Text_stem\"] = training[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "validation[\"Tweet_Text_stem\"] = validation[\"Tweet_Text\"].apply(lambda x: stemming(x))\n",
    "\n",
    "# vectorize data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train    = vectorizer.fit_transform(training['Tweet_Text_stem'])\n",
    "X_validate = vectorizer.fit_transform(validation['Tweet_Text_stem'])\n",
    "\n",
    "# Select target value\n",
    "y_train    = training[\"Sentiment\"]\n",
    "y_validate = validation[\"Sentiment\"]\n",
    "\n",
    "# Select 10 best features based on chi squared test.\n",
    "chi_square = SelectKBest(chi2)\n",
    "\n",
    "X_train    = chi_square.fit_transform(X_train, y_train)\n",
    "X_validate = chi_square.transform(X_validate)\n",
    "\n",
    "# Apply RandomForestClassifier on features\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Predictions_lemma = clf.predict(X_validate)\n",
    "\n",
    "# Add Results to DataFrame with all outcomes \n",
    "results = results.append({\"Type\"      : \"LowerCase X StopWords X Stemming\",\n",
    "                \"Precision\" : precision_score(y_validate, Predictions_lemma),\n",
    "                \"Recall\"    : recall_score(y_validate, Predictions_lemma),\n",
    "                \"F1_score\"  : f1_score(y_validate, Predictions_lemma)},\n",
    "              ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b2e2ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.998643</td>\n",
       "      <td>0.249431</td>\n",
       "      <td>0.399163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LowerCase</td>\n",
       "      <td>0.998367</td>\n",
       "      <td>0.325684</td>\n",
       "      <td>0.491147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StopWords</td>\n",
       "      <td>0.998062</td>\n",
       "      <td>0.249334</td>\n",
       "      <td>0.398993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stemming</td>\n",
       "      <td>0.998440</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.541959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LowerCase X Stopwords</td>\n",
       "      <td>0.997414</td>\n",
       "      <td>0.317502</td>\n",
       "      <td>0.481675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LowerCase X Stopwords</td>\n",
       "      <td>0.997414</td>\n",
       "      <td>0.317502</td>\n",
       "      <td>0.481675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LowerCase X Stemming</td>\n",
       "      <td>0.998440</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.541959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stopwords X Stemming</td>\n",
       "      <td>0.997511</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.538320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LowerCase X StopWords X Stemming</td>\n",
       "      <td>0.997511</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.538320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Type  Precision    Recall  F1_score\n",
       "0                          Baseline   0.998643  0.249431  0.399163\n",
       "1                         LowerCase   0.998367  0.325684  0.491147\n",
       "2                         StopWords   0.998062  0.249334  0.398993\n",
       "3                          Stemming   0.998440  0.371920  0.541959\n",
       "4             LowerCase X Stopwords   0.997414  0.317502  0.481675\n",
       "5             LowerCase X Stopwords   0.997414  0.317502  0.481675\n",
       "6              LowerCase X Stemming   0.998440  0.371920  0.541959\n",
       "7              Stopwords X Stemming   0.997511  0.368627  0.538320\n",
       "8  LowerCase X StopWords X Stemming   0.997511  0.368627  0.538320"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
